{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Native Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "Two of the previous notebooks showed how to [execute semantic functions inline](./03-semantic-function-inline.ipynb) and how to [run prompts from a file](./02-running-prompts-from-file.ipynb).\n",
    "\n",
    "In this notebook, we'll show how to use native functions from a file. We will also show how to call semantic functions from native functions.\n",
    "\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "- Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "- Using external data sources to gather data to concatenate into your prompt.\n",
    "- Validating user input data prior to sending it to the LLM prompt.\n",
    "\n",
    "Native functions are defined using standard Python code. The structure is simple, but not well documented at this point.\n",
    "\n",
    "The following examples are intended to help guide new users towards successful native & semantic function use with the SK Python framework.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da651d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.5.1.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddb5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd150646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "if selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    service_id = \"aoai_chat\"  # used later in the notebook\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        service_id=service_id, deployment_name=\"gpt-35-turbo\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "    kernel.add_service(azure_chat_service)\n",
    "\n",
    "# Configure OpenAI service\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    service_id = \"oai_chat\"  # used later in the notebook\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        service_id=service_id, ai_model_id=\"gpt-4-turbo-1106\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "    kernel.add_service(oai_chat_service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "186767f8",
   "metadata": {},
   "source": [
    "Let's create a **native** function that gives us a random number between 3 and a user input as the upper limit. We'll use this number to create 3-x paragraphs of text when passed to a semantic function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "First, let's create our native function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\",\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(3, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Next, let's create a semantic function that accepts a number as `{{$input}}` and generates that number of paragraphs about two Corgis on an adventure. `$input` is a default variable semantic functions can use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7890943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long. It must be this length.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo-1106\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"story\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.create_function_from_prompt(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.import_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2471c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberThreeOrHigher\"]\n",
    "number_result = await generate_number_three_or_higher(kernel, input=6)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f043a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await corgi_story.invoke(kernel, input=number_result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e7a2",
   "metadata": {},
   "source": [
    "_Note: depending on which model you're using, it may not respond with the proper number of paragraphs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a60e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 6 paragraphs long.\n",
      "=====================================================\n",
      "Once upon a time in a quaint little town, there lived two adventurous Corgis named Max and Ruby. Max and Ruby loved exploring the world around them, and one sunny day, they decided to embark on a grand adventure to the nearby forest. As they trotted through the tall trees and lush greenery, their tails wagged with excitement.\n",
      "\n",
      "As they journeyed deeper into the forest, they encountered all sorts of new and interesting sights and smells. They met friendly squirrels, playful rabbits, and even a wise old owl perched high in the trees. Max and Ruby were overjoyed by the beauty of the natural world and the kindness of the creatures they met along the way.\n",
      "\n",
      "As the sun began to set, Max and Ruby realized they had ventured quite far from home. They looked at each other with concern, but then remembered the valuable lesson their parents had taught them - always stick together and help each other out. With newfound determination, they set out on the journey back home, supporting each other every step of the way.\n",
      "\n",
      "Finally, with tired but happy hearts, Max and Ruby arrived back in their cozy little town. They realized that no matter where they went or what adventures they had, the most important thing was that they had each other. With a wag of their tails and a bark of joy, they knew that they could conquer any challenge as long as they faced it together. And so, Max and Ruby's grand adventure had not only brought them closer to nature but also closer to each other.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story exactly {number_result.value} paragraphs long.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef29d16",
   "metadata": {},
   "source": [
    "## Kernel Functions with Annotated Parameters\n",
    "\n",
    "That works! But let's expand on our example to make it more generic.\n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of the Python's `Annotated` class to hold these variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d54983d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    OpenAIChatCompletion,\n",
    ")\n",
    "\n",
    "if sys.version_info >= (3, 9):\n",
    "    from typing import Annotated\n",
    "else:\n",
    "    from typing_extensions import Annotated\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "if selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    service_id = \"aoai_chat\"  # used later in the notebook\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        service_id=service_id, deployment_name=\"gpt-35-turbo\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "    kernel.add_service(azure_chat_service)\n",
    "\n",
    "# Configure OpenAI service\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    service_id = \"oai_chat\"  # used later in the notebook\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        service_id=service_id, ai_model_id=\"gpt-4-turbo-1106\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "    kernel.add_service(oai_chat_service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "091f45e4",
   "metadata": {},
   "source": [
    "Let's start with the native function. Notice that we're add the `@kernel_function` decorator that holds the name of the function as well as an optional description. The input parameters are configured as part of the function's signature, and we use the `Annotated` type to specify the required input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ea462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"GenerateNumber\",\n",
    "        description=\"Generate a random number between min and max\",\n",
    "    )\n",
    "    def generate_number(\n",
    "        self,\n",
    "        min: Annotated[int, \"the minimum number of paragraphs\"],\n",
    "        max: Annotated[int, \"the maximum number of paragraphs\"] = 10,\n",
    "    ) -> Annotated[int, \"the output is a number\"]:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(min, max))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {min} and {max}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48bcdf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plugin_name='GenerateNumberPlugin' description='Generate a random number between min and max' name='GenerateNumber' is_prompt=False stream_function=None parameters=[KernelParameterMetadata(name='min', description='the minimum number of paragraphs', default_value=None, type_='int', required=True, expose=True), KernelParameterMetadata(name='max', description='the maximum number of paragraphs', default_value=10, type_='int', required=True, expose=True)] return_parameter=KernelParameterMetadata(name='return', description='the output is a number', default_value=None, type_='int', required=True, expose=False) function=<bound method GenerateNumberPlugin.generate_number of <__main__.GenerateNumberPlugin object at 0x7fc317bfbe50>> plugins=KernelPluginCollection(plugins={'GenerateNumberPlugin': KernelPlugin(name='GenerateNumberPlugin', description=None, functions={'GenerateNumber': KernelFunction(plugin_name='GenerateNumberPlugin', description='Generate a random number between min and max', name='GenerateNumber', is_prompt=False, stream_function=None, parameters=[KernelParameterMetadata(name='min', description='the minimum number of paragraphs', default_value=None, type_='int', required=True, expose=True), KernelParameterMetadata(name='max', description='the maximum number of paragraphs', default_value=10, type_='int', required=True, expose=True)], return_parameter=KernelParameterMetadata(name='return', description='the output is a number', default_value=None, type_='int', required=True, expose=False), function=<bound method GenerateNumberPlugin.generate_number of <__main__.GenerateNumberPlugin object at 0x7fc317bfbe50>>, plugins=KernelPluginCollection(plugins={...}), prompt_execution_settings={}, prompt_template_config=None, metadata=KernelFunctionMetadata(name='GenerateNumber', plugin_name='GenerateNumberPlugin', description='Generate a random number between min and max', parameters=[KernelParameterMetadata(name='min', description='the minimum number of paragraphs', default_value=None, type_='int', required=True, expose=True), KernelParameterMetadata(name='max', description='the maximum number of paragraphs', default_value=10, type_='int', required=True, expose=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='the output is a number', default_value=None, type_='int', required=True, expose=False)))})}) prompt_execution_settings={} prompt_template_config=None metadata=KernelFunctionMetadata(name='GenerateNumber', plugin_name='GenerateNumberPlugin', description='Generate a random number between min and max', parameters=[KernelParameterMetadata(name='min', description='the minimum number of paragraphs', default_value=None, type_='int', required=True, expose=True), KernelParameterMetadata(name='max', description='the maximum number of paragraphs', default_value=10, type_='int', required=True, expose=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='the output is a number', default_value=None, type_='int', required=True, expose=False))\n"
     ]
    }
   ],
   "source": [
    "generate_number_plugin = kernel.import_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")\n",
    "generate_number = generate_number_plugin[\"GenerateNumber\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad068d6",
   "metadata": {},
   "source": [
    "Now let's also allow the semantic function to take in additional arguments. In this case, we're going to allow the our CorgiStory function to be written in a specified language. We'll need to provide a `paragraph_count` and a `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b8286fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo-1106\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"summarize\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.create_function_from_prompt(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8778bad",
   "metadata": {},
   "source": [
    "Let's generate a paragraph count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28820d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 3 paragraphs long.\n"
     ]
    }
   ],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value\n",
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a9147",
   "metadata": {},
   "source": [
    "We can now invoke our corgi_story function using the `kernel` and the keyword arguments `paragraph_count` and `language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbe07c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "desired_language = \"Spanish\"\n",
    "story = await corgi_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6732a30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 3 paragraphs long in Spanish.\n",
      "=====================================================\n",
      "Había una vez dos corgis llamados Coco y Tito. Eran los mejores amigos y les encantaba ir en aventuras juntos. Un día, decidieron explorar el bosque cercano a su casa. Se adentraron en el bosque y descubrieron un hermoso arroyo donde jugaron y se divirtieron durante horas.\n",
      "\n",
      "Mientras exploraban, Coco y Tito se encontraron con un pajarito herido. Sin dudarlo, decidieron llevarlo a casa y cuidarlo hasta que estuviera completamente recuperado. A medida que pasaban los días, el pajarito se recuperó y se convirtió en un amigo más para Coco y Tito. Juntos, disfrutaron de muchas aventuras y aprendieron que la amistad y la compasión son los valores más importantes en la vida.\n",
      "\n",
      "Coco y Tito demostraron que la verdadera amistad y el amor por los demás pueden hacer del mundo un lugar mejor. Aprendieron que siempre hay espacio para ayudar a los demás y que las aventuras son mucho más emocionantes cuando se comparten con amigos. Y así, Coco, Tito y su nuevo amigo alado siguieron explorando el mundo juntos, llenando cada día de alegría y bondad.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb786c54",
   "metadata": {},
   "source": [
    "## Calling Native Functions within a Semantic Function\n",
    "\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Prompt Functions!\n",
    "\n",
    "We will make our CorgiStory semantic function call a native function `GenerateNames` which will return names for our Corgi characters.\n",
    "\n",
    "We do this using the syntax `{{plugin_name.function_name}}`. You can read more about our prompte templating syntax [here](../../../docs/PROMPT_TEMPLATE_LANGUAGE.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d84c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNamesPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @kernel_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @kernel_function(description=\"Generate character names\", name=\"generate_names\")\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\"Hoagie\", \"Hamilton\", \"Bacon\", \"Pizza\", \"Boots\", \"Shorts\", \"Tuna\"}\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ab7d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_plugin = kernel.import_plugin(GenerateNamesPlugin(), plugin_name=\"GenerateNames\")\n",
    "generate_names = generate_names_plugin[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94decd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "- The two names of the corgis are {{GenerateNames.generate_names}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be72a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo-1106\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"corgi-new\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.create_function_from_prompt(\n",
    "    function_name=\"CorgiStoryUpdated\",\n",
    "    plugin_name=\"CorgiPluginUpdated\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56e6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e980348",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_language = \"French\"\n",
    "story = await corgi_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4ade048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 5 paragraphs long in French.\n",
      "=====================================================\n",
      "Il était une fois, dans une petite ville pittoresque, deux adorables corgis nommés Bacon et Shorts. Ces deux petits chiens étaient inséparables et aimaient partir à l'aventure ensemble.\n",
      "\n",
      "Un jour, Bacon et Shorts décidèrent de partir explorer la forêt voisine. Ils couraient joyeusement à travers les sentiers, s'arrêtant de temps en temps pour renifler les fleurs sauvages et regarder les oiseaux dans les arbres.\n",
      "\n",
      "Alors qu'ils marchaient, ils rencontrèrent un lapin effrayé qui avait perdu son chemin. Sans hésiter, Bacon et Shorts s'approchèrent du lapin et lui offrirent leur aide. Ensemble, ils guidèrent le lapin de retour à son terrier, où sa famille l'attendait avec impatience.\n",
      "\n",
      "Le lapin était tellement reconnaissant qu'il invita Bacon et Shorts à un festin dans la clairière. Les corgis furent enchantés par la gentillesse du lapin et par la délicieuse nourriture qu'il leur offrit. Ils réalisèrent que même les plus petites actions de bonté peuvent apporter de la joie à ceux qui en ont besoin.\n",
      "\n",
      "Après cette aventure, Bacon et Shorts rentrèrent chez eux, le cœur rempli de gratitude. Ils savaient qu'ils avaient fait une différence dans la vie du lapin et qu'ils avaient vécu une belle aventure ensemble. Ils savaient aussi que peu importe la taille de quelqu'un, il était important d'être gentil et serviable envers tout le monde.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42f0c472",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A quick review of what we've learned here:\n",
    "\n",
    "- We've learned how to create native and prompt functions and register them to the kernel\n",
    "- We've seen how we can use Kernel Arguments to pass in more custom variables into our prompt\n",
    "- We've seen how we can call native functions within a prompt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
