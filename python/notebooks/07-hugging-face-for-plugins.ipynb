{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {},
   "source": [
    "# Using Hugging Face With Plugins\n",
    "\n",
    "In this notebook, we demonstrate using Hugging Face models for Plugins using both SemanticMemory and text completions. \n",
    "\n",
    "SK supports downloading models from the Hugging Face that can perform the following tasks: text-generation, text2text-generation, summarization, and sentence-similarity. You can search for models by task at https://huggingface.co/models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.5.1.dev0\n",
    "\n",
    "# Note that additional dependencies are required for the Hugging Face connectors:\n",
    "!python -m pip install torch==2.0.0\n",
    "!python -m pip install transformers==^4.28.1\n",
    "!python -m pip install sentence-transformers==^2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.hugging_face as sk_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753ab756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "First, we will create a kernel and add both text completion and embedding services. \n",
    "\n",
    "For text completion, we are choosing GPT2. This is a text-generation model. (Note: text-generation will repeat the input in the output, text2text-generation will not.)\n",
    "For embeddings, we are using sentence-transformers/all-MiniLM-L6-v2. Vectors generated for this model are of length 384 (compared to a length of 1536 from OpenAI ADA).\n",
    "\n",
    "The following step may take a few minutes when run for the first time as the models will be downloaded to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure LLM service\n",
    "if selectedService == Service.HuggingFace:\n",
    "    text_service_id = \"gpt2\"\n",
    "    kernel.add_service(\n",
    "        service=sk_hf.HuggingFaceTextCompletion(\n",
    "            service_id=text_service_id, ai_model_id=text_service_id, task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    embed_service_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_svc = sk_hf.HuggingFaceTextEmbedding(service_id=embed_service_id, ai_model_id=embed_service_id)\n",
    "    kernel.add_service(\n",
    "        service=embedding_svc,\n",
    "    )\n",
    "    kernel.use_memory(storage=sk.memory.VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
    "    kernel.import_plugin(sk.core_plugins.TextMemoryPlugin(), \"TextMemoryPlugin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a7e7ca4",
   "metadata": {},
   "source": [
    "### Add Memories and Define a plugin to use them\n",
    "\n",
    "Most models available on huggingface.co are not as powerful as OpenAI GPT-3+. Your plugins will likely need to be simpler to accommodate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d096504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info1\", text=\"Sharks are fish.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info2\", text=\"Whales are mammals.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info3\", text=\"Penguins are birds.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info4\", text=\"Dolphins are mammals.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info5\", text=\"Flies are insects.\")\n",
    "\n",
    "# Define prompt function using SK prompt template language\n",
    "my_prompt = \"\"\"I know these animal facts: {{recall $query1}} {{recall $query2}} {{recall $query3}} and \"\"\"\n",
    "\n",
    "execution_settings = sk_hf.HuggingFacePromptExecutionSettings(\n",
    "    service_id=text_service_id,\n",
    "    ai_model_id=text_service_id,\n",
    "    max_tokens=45,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=my_prompt,\n",
    "    name=\"text_complete\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "my_function = kernel.create_function_from_prompt(prompt_template_config=prompt_template_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2calf857",
   "metadata": {},
   "source": [
    "Let's now see what the completion looks like! Remember, \"gpt2\" is nowhere near as large as ChatGPT, so expect a much simpler answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628c843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 21, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Error occurred while invoking function f_HSMshmxNdvxpFZSl: ('Hugging Face completion failed', ValueError('If `eos_token_id` is defined, make sure that `pad_token_id` is defined.'), None)\n",
      "Error occurred while invoking function f_HSMshmxNdvxpFZSl: ('Hugging Face completion failed', ValueError('If `eos_token_id` is defined, make sure that `pad_token_id` is defined.'), None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 completed prompt with: ''\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "arguments = KernelArguments(\n",
    "    query1=\"animal that swims\",\n",
    "    query2=\"animal that flies\",\n",
    "    query3=\"penguins are?\",\n",
    "    collection=\"animal-facts\",\n",
    "    relevance=0.3,\n",
    ")\n",
    "\n",
    "output = await kernel.invoke(\n",
    "    my_function,\n",
    "    arguments,\n",
    ")\n",
    "\n",
    "output = str(output).strip()\n",
    "\n",
    "query_result1 = await kernel.memory.search(\n",
    "    collection=\"animal-facts\", query=arguments[\"query1\"], limit=1, min_relevance_score=0.3\n",
    ")\n",
    "query_result2 = await kernel.memory.search(\n",
    "    collection=\"animal-facts\", query=arguments[\"query2\"], limit=1, min_relevance_score=0.3\n",
    ")\n",
    "query_result3 = await kernel.memory.search(\n",
    "    collection=\"animal-facts\", query=arguments[\"query3\"], limit=1, min_relevance_score=0.3\n",
    ")\n",
    "\n",
    "print(f\"{text_service_id} completed prompt with: '{output}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
