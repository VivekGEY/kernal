{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {},
   "source": [
    "# Multiple Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb81bacd",
   "metadata": {},
   "source": [
    "In this notebook we show how you can in a single request, have the LLM model return multiple results per prompt. This is useful for running experiments where you want to evaluate the robustness of your prompt and the parameters of your config against a particular large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.4.4.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.ai_request_settings import AIRequestSettings\n",
    "from semantic_kernel.connectors.ai.open_ai.request_settings.open_ai_request_settings import (\n",
    "    OpenAITextRequestSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, AzureChatCompletion, OpenAITextCompletion, OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "First, we will set up the text and chat services we will be submitting prompts to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure Azure LLM service\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "azure_text_service = AzureTextCompletion(deployment_name=\"text-davinci-002\", endpoint=endpoint, api_key=api_key)    # set the deployment name to the value of your text model\n",
    "azure_chat_service = AzureChatCompletion(deployment_name=\"gpt-35-turbo\", endpoint=endpoint, api_key=api_key)   # set the deployment name to the value of your chat model\n",
    "\n",
    "\n",
    "# Configure OpenAI service\n",
    "api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "oai_text_service = OpenAITextCompletion(ai_model_id=\"text-davinci-003\", api_key=api_key, org_id=org_id)\n",
    "oai_chat_service = OpenAIChatCompletion(ai_model_id=\"gpt-3.5-turbo\", api_key=api_key, org_id=org_id)\n",
    "\n",
    "# Configure Hugging Face service\n",
    "hf_text_service = HuggingFaceTextCompletion(ai_model_id=\"distilgpt2\", task=\"text-generation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50561d82",
   "metadata": {},
   "source": [
    "Next, we'll set up the completion request settings for text completion services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_settings = OpenAITextRequestSettings(extension_data = { \"max_tokens\": 80, \"temperature\": 0.7, \"top_p\": 1, \"frequency_penalty\": 0.5, \"presence_penalty\": 0.5, \"number_of_responses\": 3})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857a9c89",
   "metadata": {},
   "source": [
    "## Multiple Open AI Text Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2979db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: \n",
      "\n",
      "The purpose of a rubber duck is primarily for use as a bath toy for children. It can also be used to teach young children basic water safety and swimming skills.\n",
      "Result 2: \n",
      "\n",
      "The purpose of a rubber duck is to provide a fun bathtime companion for children and give them something to focus on while they are learning how to bathe. Many people also use rubber ducks as decorative items in the home or as a gift.\n",
      "Result 3: \n",
      "\n",
      "A rubber duck is often used as a toy, but it can also serve several other purposes. Some people use them to help children learn about water safety, or as a calming tool for young children to help them relax during bath time. They can provide a fun and safe way for children to play in the water, and they are also popular bathroom decorations.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the purpose of a rubber duck?\"\n",
    "results = await oai_text_service.complete_async(prompt=prompt, settings=request_settings)\n",
    "i = 1\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4288d09f",
   "metadata": {},
   "source": [
    "## Multiple Azure Open AI Text Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5319f14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AIException",
     "evalue": "(<ErrorCodes.ServiceError: 6>, \"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.AzureTextCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/semantic-kernel/python/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:57\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[0;34m(self, request_settings)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m (\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_settings\u001b[38;5;241m.\u001b[39mprepare_settings_dict())\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mCHAT\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_settings\u001b[38;5;241m.\u001b[39mprepare_settings_dict())\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_usage(response)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/openai/resources/completions.py:1133\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1132\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m AsyncStream[Completion]:\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1136\u001b[0m             {\n\u001b[1;32m   1137\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1138\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   1139\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_of,\n\u001b[1;32m   1140\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecho\u001b[39m\u001b[38;5;124m\"\u001b[39m: echo,\n\u001b[1;32m   1141\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1142\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1143\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1145\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1146\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1147\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1148\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1149\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1150\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[1;32m   1151\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1152\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1153\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1154\u001b[0m             },\n\u001b[1;32m   1155\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1156\u001b[0m         ),\n\u001b[1;32m   1157\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1158\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1159\u001b[0m         ),\n\u001b[1;32m   1160\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mCompletion,\n\u001b[1;32m   1161\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1162\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[Completion],\n\u001b[1;32m   1163\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1536\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1533\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1534\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1535\u001b[0m )\n\u001b[0;32m-> 1536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1315\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1308\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1316\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1317\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1318\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1319\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1320\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1321\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/openai/_base_client.py:1392\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1395\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1396\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1399\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1400\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAIException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide me a list of possible meanings for the acronym \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORLD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m azure_text_service\u001b[38;5;241m.\u001b[39mcomplete_async(prompt\u001b[38;5;241m=\u001b[39mprompt, settings\u001b[38;5;241m=\u001b[39mrequest_settings)\n\u001b[1;32m      3\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/workspace/semantic-kernel/python/semantic_kernel/connectors/ai/open_ai/services/open_ai_text_completion_base.py:47\u001b[0m, in \u001b[0;36mOpenAITextCompletionBase.complete_async\u001b[0;34m(self, prompt, settings, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[0;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_settings\u001b[38;5;241m=\u001b[39msettings)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, Completion):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/semantic-kernel/python/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:77\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[0;34m(self, request_settings)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AIException(\n\u001b[1;32m     72\u001b[0m         AIException\u001b[38;5;241m.\u001b[39mErrorCodes\u001b[38;5;241m.\u001b[39mServiceError,\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m         ex,\n\u001b[1;32m     75\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AIException(\n\u001b[1;32m     78\u001b[0m         AIException\u001b[38;5;241m.\u001b[39mErrorCodes\u001b[38;5;241m.\u001b[39mServiceError,\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m         ex,\n\u001b[1;32m     81\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[0;31mAIException\u001b[0m: (<ErrorCodes.ServiceError: 6>, \"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.AzureTextCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}\"))"
     ]
    }
   ],
   "source": [
    "prompt = \"provide me a list of possible meanings for the acronym 'ORLD'\"\n",
    "results = await azure_text_service.complete_async(prompt=prompt, settings=request_settings)\n",
    "i = 1\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb548f9c",
   "metadata": {},
   "source": [
    "## Multiple Hugging Face Text Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a148709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.hugging_face.hf_request_settings import (\n",
    "    HuggingFaceRequestSettings,\n",
    ")\n",
    "\n",
    "request_settings = HuggingFaceRequestSettings(extension_data = { \"max_new_tokens\": 80, \"temperature\": 0.7, \"top_p\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9525e4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AIException",
     "evalue": "('Hugging Face completion failed', TypeError('HuggingFaceRequestSettings.prepare_settings_dict() takes 1 positional argument but 2 were given'), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/semantic-kernel/python/semantic_kernel/connectors/ai/hugging_face/services/hf_text_completion.py:84\u001b[0m, in \u001b[0;36mHuggingFaceTextCompletion.complete_async\u001b[0;34m(self, prompt, request_settings, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mrequest_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_settings_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     85\u001b[0m     result_field_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: HuggingFaceRequestSettings.prepare_settings_dict() takes 1 positional argument but 2 were given",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAIException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe purpose of a rubber duck is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m hf_text_service\u001b[38;5;241m.\u001b[39mcomplete_async(prompt\u001b[38;5;241m=\u001b[39mprompt, request_settings\u001b[38;5;241m=\u001b[39mrequest_settings)\n\u001b[1;32m      3\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/workspace/semantic-kernel/python/semantic_kernel/connectors/ai/hugging_face/services/hf_text_completion.py:91\u001b[0m, in \u001b[0;36mHuggingFaceTextCompletion.complete_async\u001b[0;34m(self, prompt, request_settings, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [resp[result_field_name] \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AIException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHugging Face completion failed\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "\u001b[0;31mAIException\u001b[0m: ('Hugging Face completion failed', TypeError('HuggingFaceRequestSettings.prepare_settings_dict() takes 1 positional argument but 2 were given'), None)"
     ]
    }
   ],
   "source": [
    "prompt = \"The purpose of a rubber duck is\"\n",
    "results = await hf_text_service.complete_async(prompt=prompt, request_settings=request_settings)\n",
    "i = 1\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da632e12",
   "metadata": {},
   "source": [
    "Here, we're setting up the settings for Chat completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f11e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_request_settings = ChatRequestSettings(\n",
    "    max_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6bf238e",
   "metadata": {},
   "source": [
    "## Multiple OpenAI Chat Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"user\"\n",
    "content = \"It's a beautiful day outside, birds are singing, flowers are blooming. On days like these, kids like you...\"\n",
    "message = { \"role\":role, \"content\":content }\n",
    "results = await oai_chat_service.complete_chat_async(messages=[message], settings=chat_request_settings)\n",
    "i = 0\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdb8f740",
   "metadata": {},
   "source": [
    "## Multiple Azure OpenAI Chat Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"user\"\n",
    "content = \"Tomorow is going to be a great day, I can feel it. I'm going to wake up early, go for a run, and then...\"\n",
    "message = { \"role\":role, \"content\":content }\n",
    "results = await azure_chat_service.complete_chat_async(messages=[message], settings=chat_request_settings)\n",
    "i = 0\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c8191d",
   "metadata": {},
   "source": [
    "## Streaming Multiple Results\n",
    "\n",
    "Here is an example pattern if you want to stream your multiple results. Note that this is not supported for Hugging Face text completions at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    clear = \"cls\"\n",
    "else:\n",
    "    clear = \"clear\"\n",
    "\n",
    "prompt = \"what is the purpose of a rubber duck?\"\n",
    "stream = oai_text_service.complete_stream_async(prompt=prompt, settings=request_settings)\n",
    "texts = [\"\"] * request_settings.number_of_responses\n",
    "async for results in stream:\n",
    "    i = 1\n",
    "    clear_output(wait=True)\n",
    "    for result in results:\n",
    "        texts[i - 1] += result\n",
    "        print(f\"Result {i}: {texts[i - 1]}\")\n",
    "        i += 1\n",
    "\n",
    "print(\"----------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
